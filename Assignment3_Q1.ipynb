{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing required Libraries\nimport os, shutil\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Flatten,Embedding,Dense,Concatenate\nfrom keras.utils.vis_utils import plot_model\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-05T13:28:44.818927Z","iopub.execute_input":"2022-05-05T13:28:44.819248Z","iopub.status.idle":"2022-05-05T13:28:49.804623Z","shell.execute_reply.started":"2022-05-05T13:28:44.819159Z","shell.execute_reply":"2022-05-05T13:28:49.803908Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Downloading the dataset from the link using wget\n!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:28:53.753363Z","iopub.execute_input":"2022-05-05T13:28:53.754009Z","iopub.status.idle":"2022-05-05T13:29:22.080374Z","shell.execute_reply.started":"2022-05-05T13:28:53.753967Z","shell.execute_reply":"2022-05-05T13:29:22.079758Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Extracting and removing the .tar file\n!tar -xf dakshina_dataset_v1.0.tar\n!rm -r dakshina_dataset_v1.0.tar","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:29:25.355515Z","iopub.execute_input":"2022-05-05T13:29:25.356269Z","iopub.status.idle":"2022-05-05T13:29:29.686233Z","shell.execute_reply.started":"2022-05-05T13:29:25.356219Z","shell.execute_reply":"2022-05-05T13:29:29.685029Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Choose the language\n# Colab\n# dir = '/content/dakshina_dataset_v1.0'\n# Kaggle\ndir = './dakshina_dataset_v1.0'\nlang = \"hi\"","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:29:34.447135Z","iopub.execute_input":"2022-05-05T13:29:34.447632Z","iopub.status.idle":"2022-05-05T13:29:34.452533Z","shell.execute_reply.started":"2022-05-05T13:29:34.447601Z","shell.execute_reply":"2022-05-05T13:29:34.451724Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Removing the unnecessary folders and keeping only the lang choosen lexicons folder\nfor files in os.listdir(dir):\n    if (files==lang):    \n        path = os.path.join(dir, files)\n        for files_lang in os.listdir(path):\n            if(files_lang!=\"lexicons\"):\n                path_lang = os.path.join(path, files_lang)\n                try:\n                    shutil.rmtree(path_lang)\n                except OSError:\n                    os.remove(path_lang)\n    else:    \n        path = os.path.join(dir, files)\n        try:\n            shutil.rmtree(path)\n        except OSError:\n            os.remove(path)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:29:43.359495Z","iopub.execute_input":"2022-05-05T13:29:43.359972Z","iopub.status.idle":"2022-05-05T13:29:43.656898Z","shell.execute_reply.started":"2022-05-05T13:29:43.359930Z","shell.execute_reply":"2022-05-05T13:29:43.656250Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Train, validation and test dataset path\ntrain_folder = \".translit.sampled.train.tsv\"\ntrain_path = os.path.join(dir, lang,\"lexicons\",lang+train_folder)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:29:46.757524Z","iopub.execute_input":"2022-05-05T13:29:46.757878Z","iopub.status.idle":"2022-05-05T13:29:46.761972Z","shell.execute_reply.started":"2022-05-05T13:29:46.757833Z","shell.execute_reply":"2022-05-05T13:29:46.761073Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Parsing the training data to find number of unique characters in input language and output language( these values are useful in creating embedding layer)\n# readData function will take path as argument and return the data present in that path as pandas DataFrame\ndef readData(path):    \n    trainingData_df = pd.read_csv(path, sep='\\t',on_bad_lines='skip',header=None)\n    trainingData = trainingData_df.values.tolist()\n    return trainingData\n\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\n\n# Reading the training Data as data frame\ntrainingData = readData(train_path)\n\n# Iterating through each training data item\nfor line in trainingData:\n    input_text, target_text = line[1],line[0]\n    if not isinstance(input_text,str):\n        continue\n    target_text = \" \" + target_text + \" \"\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)\n# input_characters list contain unique characters in input language \ninput_characters.add(' ')\n# target_characters list contain unique characters in target language \ntarget_characters.add(' ')\ninput_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\n\n\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\n# max_encoder_seq_length is maximum length word in input language\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\n# max_decoder_seq_length is maximum length word in target language\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:29:49.559474Z","iopub.execute_input":"2022-05-05T13:29:49.559724Z","iopub.status.idle":"2022-05-05T13:29:49.766866Z","shell.execute_reply.started":"2022-05-05T13:29:49.559696Z","shell.execute_reply":"2022-05-05T13:29:49.766033Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"'''\nbuilModel will take \n    \n    latent_dims - list of hidden states sizes, list length gives number of encoders and decoder present in the model\n    EmbeddingOutputDimensions - [encoder embedding output size, decoder embedding output size]\n    layer_type - type of cell used in encoder and decoder (rnn or lstm or gru)\n    \n    as arguments and return prepared model\n'''\n\ndef buildModel(latent_dims,EmbeddingOutputDimensions,layer_type):\n    encoder_inputs = keras.Input(shape=(None,)) # input layer\n    embedding_encoder_layer = Embedding(input_dim = num_encoder_tokens, output_dim = EmbeddingOutputDimensions[0], trainable=True) # embedding layer\n    embedding_encoder_inuts = embedding_encoder_layer(encoder_inputs)\n    encoder_outputs = embedding_encoder_inuts\n    \n    encoder_states = [] # encoder_states stores last hidden states return by each cell in encoder to provide as input to respective decoder cells \n    \n    # this loop creats layer_type cells with configuration present in latent_dims list\n    for i in range(len(latent_dims))[::-1]:\n        if layer_type == 'lstm':\n            encoder_outputs, state_h, state_c = keras.layers.LSTM(latent_dims[i], return_state=True,return_sequences=True)(encoder_outputs)\n            encoder_states += [state_h, state_c]\n        if layer_type == 'gru':\n            encoder_outputs, state_h= keras.layers.GRU(latent_dims[i], return_state=True, return_sequences=True)(encoder_outputs)\n            encoder_states += [state_h]\n        if layer_type == 'rnn':\n            encoder_outputs, state_h = keras.layers.SimpleRNN(latent_dims[i], return_state=True, return_sequences=True)(encoder_outputs)\n            encoder_states += [state_h]\n\n    decoder_inputs = keras.Input(shape=(None,)) # input layer\n    embedding_decoder_layer = Embedding(input_dim = num_decoder_tokens, output_dim = EmbeddingOutputDimensions[1],trainable=True) # embedding layer\n    embedding_decoder_inputs = embedding_decoder_layer(decoder_inputs)\n    decoder_outputs_temp = embedding_decoder_inputs\n    \n    # this loop creats layer_type cells with configuration present in latent_dims list\n    for i in range(len(latent_dims)):\n        if layer_type == 'lstm':\n            decoder_outputs_temp, dh, dc = keras.layers.LSTM(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True)(decoder_outputs_temp, initial_state=encoder_states[2*i:2*(i+1)])\n        if layer_type == 'gru':\n            decoder_outputs_temp,dh = keras.layers.GRU(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True)(decoder_outputs_temp, initial_state=encoder_states[i])\n        if layer_type == 'rnn':\n            decoder_outputs_temp, dh = keras.layers.SimpleRNN(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True)(decoder_outputs_temp, initial_state=encoder_states[i])\n     \n    dense_layer = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\") #dense layer\n    decoder_outputs = dense_layer(decoder_outputs_temp) \n    \n    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs) # creating model\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:29:52.697258Z","iopub.execute_input":"2022-05-05T13:29:52.697971Z","iopub.status.idle":"2022-05-05T13:29:52.713236Z","shell.execute_reply.started":"2022-05-05T13:29:52.697924Z","shell.execute_reply":"2022-05-05T13:29:52.712425Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"'''\nFormat of command line arguments\n1) number of cells in encoder or decoder (let n)\n2 to n) hidden states sizes\nn+1) encoder embedding output size\nn+2) decoder embedding output size\nn+3) cell type (rnn or lstm or gru)\n'''\n# number_of_en_de = int(sys.argv[1])\n# latent_dims = []\n# for i in range(0,number_of_en_de):\n#     latent_dims.append(int(sys.argv[i+2]))\n# embed_dims = [int(sys.argv[number_of_en_de+2]),int(sys.argv[number_of_en_de+3])]\n# cell_type = sys.argv(number_of_en_de+4)\n\n\nnumber_of_en_de = 3\nlatent_dims = [256,512,256]\nembed_dims = [256,256]\ncell_type = 'lstm'\n\n\n# build model\nmodel = buildModel(latent_dims=latent_dims,EmbeddingOutputDimensions=embed_dims,layer_type=cell_type)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:29:55.635419Z","iopub.execute_input":"2022-05-05T13:29:55.636205Z","iopub.status.idle":"2022-05-05T13:29:57.625436Z","shell.execute_reply.started":"2022-05-05T13:29:55.636164Z","shell.execute_reply":"2022-05-05T13:29:57.624760Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:30:04.237302Z","iopub.execute_input":"2022-05-05T13:30:04.237537Z","iopub.status.idle":"2022-05-05T13:30:05.193739Z","shell.execute_reply.started":"2022-05-05T13:30:04.237512Z","shell.execute_reply":"2022-05-05T13:30:05.192934Z"},"trusted":true},"execution_count":10,"outputs":[]}]}
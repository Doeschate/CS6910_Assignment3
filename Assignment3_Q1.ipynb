{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing required Libraries\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Flatten,Embedding,Dense,Concatenate\nfrom keras.utils.vis_utils import plot_model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-05T12:46:19.835015Z","iopub.execute_input":"2022-05-05T12:46:19.835400Z","iopub.status.idle":"2022-05-05T12:46:26.409157Z","shell.execute_reply.started":"2022-05-05T12:46:19.835296Z","shell.execute_reply":"2022-05-05T12:46:26.407799Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Downloading the Data\n!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n!tar -xvf ./dakshina_dataset_v1.0.tar","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:48:05.824691Z","iopub.execute_input":"2022-05-05T12:48:05.825514Z","iopub.status.idle":"2022-05-05T12:48:25.307909Z","shell.execute_reply.started":"2022-05-05T12:48:05.825463Z","shell.execute_reply":"2022-05-05T12:48:25.306978Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Parsing the training data to find number of unique characters in input language and output language( these values are useful in creating embedding layer)\n\ntrain_path = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n\n# readData function will take path as argument and return the data present in that path as pandas DataFrame\ndef readData(path):    \n    trainingData_df = pd.read_csv(path, sep='\\t',on_bad_lines='skip',header=None)\n    trainingData = trainingData_df.values.tolist()\n    return trainingData\n\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\n\n# Reading the training Data as data frame\ntrainingData = readData(train_path)\n\n# Iterating through each training data item\nfor line in trainingData:\n    input_text, target_text = line[1],line[0]\n    if not isinstance(input_text,str):\n        continue\n    target_text = \" \" + target_text + \" \"\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)\n# input_characters list contain unique characters in input language \ninput_characters.add(' ')\n# target_characters list contain unique characters in target language \ntarget_characters.add(' ')\ninput_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\n\n\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\n# max_encoder_seq_length is maximum length word in input language\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\n# max_decoder_seq_length is maximum length word in target language\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:48:38.036793Z","iopub.execute_input":"2022-05-05T12:48:38.038490Z","iopub.status.idle":"2022-05-05T12:48:38.427782Z","shell.execute_reply.started":"2022-05-05T12:48:38.038374Z","shell.execute_reply":"2022-05-05T12:48:38.426362Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"'''\nbuilModel will take \n    \n    latent_dims - list of hidden states sizes, list length gives number of encoders and decoder present in the model\n    EmbeddingOutputDimensions - [encoder embedding output size, decoder embedding output size]\n    layer_type - type of cell used in encoder and decoder (rnn or lstm or gru)\n    \n    as arguments and return prepared model\n'''\n\ndef buildModel(latent_dims,EmbeddingOutputDimensions,layer_type):\n    encoder_inputs = keras.Input(shape=(None,)) # input layer\n    embedding_encoder_layer = Embedding(input_dim = num_encoder_tokens, output_dim = EmbeddingOutputDimensions[0], trainable=True) # embedding layer\n    embedding_encoder_inuts = embedding_encoder_layer(encoder_inputs)\n    encoder_outputs = embedding_encoder_inuts\n    \n    encoder_states = [] # encoder_states stores last hidden states return by each cell in encoder to provide as input to respective decoder cells \n    \n    # this loop creats layer_type cells with configuration present in latent_dims list\n    for i in range(len(latent_dims))[::-1]:\n        if layer_type == 'lstm':\n            encoder_outputs, state_h, state_c = keras.layers.LSTM(latent_dims[i], return_state=True,return_sequences=True)(encoder_outputs)\n            encoder_states += [state_h, state_c]\n        if layer_type == 'gru':\n            encoder_outputs, state_h= keras.layers.GRU(latent_dims[i], return_state=True, return_sequences=True)(encoder_outputs)\n            encoder_states += [state_h]\n        if layer_type == 'rnn':\n            encoder_outputs, state_h = keras.layers.SimpleRNN(latent_dims[i], return_state=True, return_sequences=True)(encoder_outputs)\n            encoder_states += [state_h]\n\n    decoder_inputs = keras.Input(shape=(None,)) # input layer\n    embedding_decoder_layer = Embedding(input_dim = num_decoder_tokens, output_dim = EmbeddingOutputDimensions[1],trainable=True) # embedding layer\n    embedding_decoder_inputs = embedding_decoder_layer(decoder_inputs)\n    decoder_outputs_temp = embedding_decoder_inputs\n    \n    # this loop creats layer_type cells with configuration present in latent_dims list\n    for i in range(len(latent_dims)):\n        if layer_type == 'lstm':\n            decoder_outputs_temp, dh, dc = keras.layers.LSTM(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True)(decoder_outputs_temp, initial_state=encoder_states[2*i:2*(i+1)])\n        if layer_type == 'gru':\n            decoder_outputs_temp,dh = keras.layers.GRU(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True)(decoder_outputs_temp, initial_state=encoder_states[i])\n        if layer_type == 'rnn':\n            decoder_outputs_temp, dh = keras.layers.SimpleRNN(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True)(decoder_outputs_temp, initial_state=encoder_states[i])\n     \n    dense_layer = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\") #dense layer\n    decoder_outputs = dense_layer(decoder_outputs_temp) \n    \n    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs) # creating model\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:48:43.115810Z","iopub.execute_input":"2022-05-05T12:48:43.116159Z","iopub.status.idle":"2022-05-05T12:48:43.137915Z","shell.execute_reply.started":"2022-05-05T12:48:43.116127Z","shell.execute_reply":"2022-05-05T12:48:43.136941Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"'''\nFormat of command line arguments\n1) number of cells in encoder or decoder (let n)\n2 to n) hidden states sizes\nn+1) encoder embedding output size\nn+2) decoder embedding output size\nn+3) cell type (rnn or lstm or gru)\n'''\n# number_of_en_de = int(sys.argv[1])\n# latent_dims = []\n# for i in range(0,number_of_en_de):\n#     latent_dims.append(int(sys.argv[i+2]))\n# embed_dims = [int(sys.argv[number_of_en_de+2]),int(sys.argv[number_of_en_de+3])]\n# cell_type = sys.argv(number_of_en_de+4)\n\n\nnumber_of_en_de = 3\nlatent_dims = [256,512,256]\nembed_dims = [256,256]\ncell_type = 'lstm'\n\n\n# build model\nmodel = buildModel(latent_dims=latent_dims,EmbeddingOutputDimensions=embed_dims,layer_type=cell_type)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:50:37.621746Z","iopub.execute_input":"2022-05-05T12:50:37.622234Z","iopub.status.idle":"2022-05-05T12:50:40.354112Z","shell.execute_reply.started":"2022-05-05T12:50:37.622191Z","shell.execute_reply":"2022-05-05T12:50:40.352795Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lkYZtD_Le3Un","trusted":true},"outputs":[],"source":["!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n","!tar -xvf ./dakshina_dataset_v1.0.tar"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:10.806081Z","iopub.status.busy":"2022-05-03T05:30:10.805747Z","iopub.status.idle":"2022-05-03T05:30:17.882998Z","shell.execute_reply":"2022-05-03T05:30:17.882011Z","shell.execute_reply.started":"2022-05-03T05:30:10.806036Z"},"id":"KWqvKpoee3U4","trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Flatten,Embedding,Dense\n","from keras.utils.vis_utils import plot_model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T02:56:47.220363Z","iopub.status.busy":"2022-05-03T02:56:47.219898Z","iopub.status.idle":"2022-05-03T02:57:03.250744Z","shell.execute_reply":"2022-05-03T02:57:03.249769Z","shell.execute_reply.started":"2022-05-03T02:56:47.220317Z"},"id":"72tH1ltv_rcQ","outputId":"c602192d-d683-47ae-bbbe-b4a03f192e71","trusted":true},"outputs":[],"source":["!pip install wandb\n","# wandb login\n","import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:17.885526Z","iopub.status.busy":"2022-05-03T05:30:17.884793Z","iopub.status.idle":"2022-05-03T05:30:17.891666Z","shell.execute_reply":"2022-05-03T05:30:17.890800Z","shell.execute_reply.started":"2022-05-03T05:30:17.885472Z"},"id":"jpy9bz8we3U-","trusted":true},"outputs":[],"source":["train_path = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n","val_path =   \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n","test_path = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n","\n","def readData(path):    \n","    trainingData_df = pd.read_csv(path, sep='\\t',on_bad_lines='skip',header=None)\n","    trainingData = trainingData_df.values.tolist()\n","    return trainingData"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:18.712220Z","iopub.status.busy":"2022-05-03T05:30:18.711329Z","iopub.status.idle":"2022-05-03T05:30:18.973695Z","shell.execute_reply":"2022-05-03T05:30:18.972552Z","shell.execute_reply.started":"2022-05-03T05:30:18.712168Z"},"id":"wMozSXGHe3VE","outputId":"4467e400-d26f-424c-90d4-5a64f775643b","trusted":true},"outputs":[],"source":["# Analysing dataset\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","\n","trainingData = readData(train_path)\n","for line in trainingData:\n","    input_text, target_text = line[1],line[0]\n","    if not isinstance(input_text,str):\n","        continue\n","    target_text = \" \" + target_text + \" \"\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","input_characters.add(' ')\n","target_characters.add(' ')\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n","\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n","\n","\n","print(input_token_index)\n","print(target_token_index)\n","\n","print(\"Number of samples:\", len(input_texts))\n","num_samples = len(input_texts)\n","print(\"Number of unique input tokens:\", num_encoder_tokens)\n","print(\"Number of unique output tokens:\", num_decoder_tokens)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:20.125127Z","iopub.status.busy":"2022-05-03T05:30:20.122611Z","iopub.status.idle":"2022-05-03T05:30:20.141120Z","shell.execute_reply":"2022-05-03T05:30:20.140124Z","shell.execute_reply.started":"2022-05-03T05:30:20.125079Z"},"id":"HLGa7pmje3VH","trusted":true},"outputs":[],"source":["# Character encoding using Embedding layer....\n","\n","# Encoder inputs embedding (Latin)\n","def getData(path):\n","    print(path)\n","    input_texts = []\n","    target_texts = []\n","    data = readData(path)\n","    for line in data:\n","        input_text, target_text = line[1],line[0]\n","        if not isinstance(input_text,str):\n","            continue\n","        target_text = \" \" + target_text + \" \"\n","        input_texts.append(input_text)\n","        target_texts.append(target_text)\n","    \n","    vocab_size = num_encoder_tokens\n","    max_length = max_encoder_seq_length\n","\n","    EncoderInputEncodedWords = []\n","    for i,eachText in enumerate(input_texts):\n","        EncoderInputEncodedWords.append([])\n","        for eachChar in eachText:\n","            EncoderInputEncodedWords[i].append(input_token_index[eachChar])\n","\n","    EncoderInputEncodedWords = pad_sequences(EncoderInputEncodedWords,maxlen=max_length,padding='post',value=0.0)\n","    print('EncoderInputEncodedWords.shape',EncoderInputEncodedWords.shape)\n","    print(EncoderInputEncodedWords[:10])\n","\n","    vocab_size = num_decoder_tokens\n","    max_length = max_decoder_seq_length\n","\n","    DecoderInputEncodedWords = []\n","    for i,eachText in enumerate(target_texts):\n","        DecoderInputEncodedWords.append([])\n","        for j,eachChar in enumerate(eachText):\n","            DecoderInputEncodedWords[i].append(target_token_index[eachChar])\n","\n","    DecoderInputEncodedWords = pad_sequences(DecoderInputEncodedWords,maxlen = max_decoder_seq_length ,padding='post',value = 0.0)#max(num_decoder_tokens,num_encoder_tokens))\n","    print('DecoderInputEncodedWords.shape',DecoderInputEncodedWords.shape)\n","    print(DecoderInputEncodedWords[:10])\n","\n","    decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n","    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","        for t, char in enumerate(target_text):\n","            if t > 0:\n","                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n","\n","    with np.printoptions(threshold=np.inf):\n","      print(decoder_target_data[0])\n","    \n","    return EncoderInputEncodedWords,DecoderInputEncodedWords,decoder_target_data,input_texts,target_texts"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:26.577682Z","iopub.status.busy":"2022-05-03T05:30:26.577327Z","iopub.status.idle":"2022-05-03T05:30:28.307514Z","shell.execute_reply":"2022-05-03T05:30:28.306545Z","shell.execute_reply.started":"2022-05-03T05:30:26.577648Z"},"id":"8Ugu7H4le3VR","trusted":true},"outputs":[],"source":["encoder_input_train_data, decoder_input_train_data, decoder_target_train_data,train_eng,train_hin = getData(train_path)\n","encoder_input_val_data, decoder_input_val_data, decoder_target_val_data,val_eng,val_hin = getData(val_path)\n","encoder_input_test_data, decoder_input_test_data, decoder_target_test_data,test_eng,test_hin = getData(test_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define an input sequence and process it.\n","def buildModel(latent_dims,EmbeddingOutputDimensions,layer_type,dropout, lr, optimiser):\n","    encoder_inputs = keras.Input(shape=(max_encoder_seq_length,))\n","    embedding_encoder_layer = Embedding(input_dim = num_encoder_tokens, output_dim = EmbeddingOutputDimensions[0], input_length = max_encoder_seq_length,trainable=True)\n","    embedding_encoder_inuts = embedding_encoder_layer(encoder_inputs)\n","    encoder_outputs = embedding_encoder_inuts\n","    \n","    encoder_states = []\n","    encoder_layers = []\n","    encoder_layers.append(encoder_inputs)\n","    encoder_layers.append(embedding_encoder_layer)\n","    for i in range(len(latent_dims))[::-1]:\n","        if layer_type == 'lstm':\n","            encoder_layers.append(keras.layers.LSTM(latent_dims[i], return_state=True,return_sequences=True,dropout=dropout))\n","            encoder_outputs, state_h, state_c = encoder_layers[-1](encoder_outputs)\n","            encoder_states += [state_h, state_c]\n","        if layer_type == 'gru':\n","            encoder_layers.append(keras.layers.GRU(latent_dims[i], return_state=True, return_sequences=True,dropout=dropout))\n","            encoder_outputs, state_h= encoder_layers[-1](encoder_outputs)\n","            encoder_states += [state_h]\n","        if layer_type == 'rnn':\n","            encoder_layers.append(keras.layers.SimpleRNN(latent_dims[i], return_state=True, return_sequences=True,dropout=dropout))\n","            encoder_outputs, state_h = encoder_layers[-1](encoder_outputs)\n","            encoder_states += [state_h]\n","\n","    decoder_inputs = keras.Input(shape=(max_decoder_seq_length,))\n","    embedding_decoder_layer = Embedding(input_dim = num_decoder_tokens, output_dim = EmbeddingOutputDimensions[1], input_length = max_decoder_seq_length,trainable=True)\n","    embedding_decoder_inputs = embedding_decoder_layer(decoder_inputs)\n","    decoder_outputs_temp = embedding_decoder_inputs\n","    decoder_layers = []\n","    decoder_layers.append(decoder_inputs)\n","    decoder_layers.append(embedding_decoder_layer)\n","\n","    for i in range(len(latent_dims)):\n","        if layer_type == 'lstm':\n","            layer = keras.layers.LSTM(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True,dropout=dropout)\n","            decoder_outputs_temp, dh, dc = layer(decoder_outputs_temp, initial_state=encoder_states[2*i:2*(i+1)])\n","            decoder_layers.append(layer)\n","        if layer_type == 'gru':\n","            layer = keras.layers.GRU(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True,dropout=dropout)\n","            decoder_outputs_temp,dh = layer(decoder_outputs_temp, initial_state=encoder_states[i])\n","            decoder_layers.append(layer)\n","        if layer_type == 'rnn':\n","            layer = keras.layers.SimpleRNN(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True,dropout=dropout)\n","            decoder_outputs_temp, dh = layer(decoder_outputs_temp, initial_state=encoder_states[i])\n","            decoder_layers.append(layer)\n","            \n","    if attention == True:\n","        attention_layer = keras.layers.AdditiveAttention() \n","        decoder_layers.append(attention_layer)\n","        attn_out, attn_states = attention_layer([ decoder_outputs_temp , encoder_outputs] , return_attention_scores = True) \n","\n","        concatenate_layer = Concatenate(axis=-1, name='concat_layer')\n","        decoder_layers.append(concatenate_layer)\n","        decoder_outputs_temp = concatenate_layer([decoder_outputs_temp, attn_out])\n","\n","\n","    dense_layer = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n","    decoder_outputs = dense_layer(decoder_outputs_temp) \n","    decoder_layers.append(dense_layer)\n","\n","    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","    #model.summary()\n","    plot_model(model, to_file='model12.png', show_shapes=True)\n","    \n","    lr = tf.keras.experimental.CosineDecayRestarts(lr , 1000)\n","    if optimiser == 'adam':\n","        optim = keras.optimizers.Adam(learning_rate=lr)\n","    elif optimiser == 'rmsprop':\n","        optim = keras.optimizers.RMSprop(learning_rate=lr)\n","    elif optimiser == 'sgd':\n","        optim = keras.optimizers.SGD(learning_rate=lr)\n","    model.compile(optimizer =optim, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","    \n","    return model,encoder_layers,decoder_layers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def beam_selector(logit , beam_width , rows ):\n","    holder = list()\n","    for i in range(len(rows)):\n","        cur_sequence, cur_score = rows[i]\n","        desired_top = np.argsort(logit)[-beam_width:]\n","    for desired in desired_top:\n","        current_holder = [cur_sequence + [desired], cur_score + tf.math.log(logit[desired])]\n","        holder.append(current_holder)\n","    return holder\n","\n","def BeamDecoder(logits, beam_width):\n","    rows = [[list(), 0.0]]\n","    logits  = tf.nn.softmax(logits)\n","    for logit in logits:\n","        holder = beam_selector(logit , beam_width , rows )\n","        rev_sorted = sorted(holder, key=lambda start_tuple:start_tuple[1], reverse=True)\n","        rows = rev_sorted[:beam_width]\n","    return np.array(rows)[:,0:1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AccuracyCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, trainingData, validationData, beam_width):\n","        self.trainingData = trainingData\n","        self.validationData = validationData   \n","        self.beamWidth = beam_width\n","\n","    def on_epoch_end(self, epoch , logs):\n","        train_preds = self.model.predict(self.trainingData[0])\n","        train_truth = self.trainingData[1]\n","        train_pred_one_hot = tf.one_hot(tf.argmax(train_preds, axis=2), train_preds.shape[2]).numpy()\n","        correct_word_count = 0\n","        correct_character_count = 0\n","\n","        for i in range(len(train_preds)):\n","            if np.array_equal(train_truth[i], train_pred_one_hot[i]):\n","                correct_word_count += 1\n","            for j in range(0,len(train_truth[i])):\n","                if np.array_equal(train_truth[i][j], train_pred_one_hot[i][j]):\n","                    correct_character_count += 1\n","\n","        trainingAccuracy_word = correct_word_count/len(train_preds)\n","        trainingAccuracy_character = correct_character_count/(len(train_preds)*len(train_preds[0]))\n","        print(\"\")\n","        print(correct_character_count,len(train_preds),len(train_preds[0]))\n","        print(str(correct_word_count),\" words correctly predicted among \",len(train_preds),\" words\")\n","        print(\"Training Accuracy (word): \"+ str(trainingAccuracy_word))\n","        print(\"Training Accuracy (character): \"+ str(trainingAccuracy_character))\n","\n","        #wandb.log({\"accuracy\": (correct_count/len(train_preds))})\n","\n","\n","        val_preds = self.model.predict(self.validationData[0])\n","        val_truth = self.validationData[1]\n","\n","        val_pred_one_hot = tf.one_hot(tf.argmax(val_preds, axis=2), val_preds.shape[2]).numpy()\n","        correct_word_count = 0\n","        correct_character_count = 0\n","        for i in range(len(val_preds)):\n","            req_length = len(val_hin[i].strip()) \n","            beam_result = BeamDecoder(np.array(val_preds[i][:req_length+1]) , self.beamWidth)\n","\n","            ground_label = decoder_input_val_data[i ,1:req_length+2]\n","\n","            maxCharMatch = 0\n","            for beam in beam_result:\n","                charMatch = 0  \n","                for j in range(0,len(ground_label)):\n","                    if beam[0][j] == ground_label[j]:\n","                        charMatch += 1\n","                    if charMatch > maxCharMatch:\n","                        maxCharMatch = charMatch\n","\n","                if(np.array_equal(beam[0] , ground_label)):\n","                    correct_word_count += 1\n","                    break\n","            correct_character_count +=  maxCharMatch\n","        validationAccuracy_word = (correct_word_count/len(val_preds))\n","        validationAccuracy_char = (correct_character_count/(len(val_preds)*len(val_preds[0])))\n","        print(correct_character_count,len(val_preds),len(val_preds[0]))\n","        print(str(correct_word_count),\" words correctly predicted among \",len(val_preds),\" words\")\n","        print(\"Validation Accuracy (word) : \"+ str(validationAccuracy_word))\n","        print(\"Validation Accuracy (character): \"+ str(validationAccuracy_char))\n","\n","        #wandb.log({ \"epoch\": epoch,\"accuracy\": trainingAccuracy_word,\"val_accuracy\": validationAccuracy_word,\"accuracy (character)\": trainingAccuracy_character, \"val_accuracy (character)\": validationAccuracy_char})\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def wandb_sweep(config=None):\n","    with wandb.init(config=config, ):\n","        config=wandb.config\n","        wandb.run.name = 'eEmS-'+str(config.encoder_embedding_size)+'-dEms-'+ str(config.decoder_embedding_size)+'-cell-'+str(config.cell_type)+ '-ldim-'+str(config.encoder_decoder_latentDims)+ '-dr-'+str(config.dropout) +'-opi-'+str(config.optimiser)+'-lr-'+str(config.learning_rate)+'-ep-'+str(config.epochs)+'-bw-'+str(config.beam_width)+'bs'+str(config.batch_size)\n","        model, encoder_layers, decoder_layers = buildModel(latent_dims=config.encoder_decoder_latentDims,EmbeddingOutputDimensions=[config.encoder_embedding_size,config.decoder_embedding_size],layer_type=config.cell_type,dropout=config.dropout, lr=config.learning_rate, optimiser=config.optimiser)\n","        beam_width = config.beam_width\n","        acc_callback = AccuracyCallback(trainingData = ([encoder_input_train_data, decoder_input_train_data], decoder_target_train_data),validationData = ([encoder_input_val_data, decoder_input_val_data], decoder_target_val_data), beam_width=beam_width)\n","\n","        model.fit([encoder_input_train_data, decoder_input_train_data],decoder_target_train_data,\n","                  batch_size=config.batch_size,\n","                  epochs=config.epochs,\n","                  shuffle=True,\n","                  callbacks=[acc_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Different sweep configurations\n","sweep_config = {\n","    \"method\":\"bayes\",\n","}\n","metric = {\n","    \"name\" : \"val_accuracy\",\n","    \"goal\" : \"maximize\"\n","}\n","sweep_config['metric']=metric\n","parameter_dict = {\n","        'encoder_embedding_size': {\n","            'values': [32,64,128,256]\n","        },\n","        'decoder_embedding_size': {\n","            'values': [32,64,128,256]\n","        },\n","        'cell_type': {\n","            'values': ['lstm', 'gru', 'rnn']\n","        },\n","        'encoder_decoder_latentDims': {\n","            'values': [ [256],[512],[64,128],[256,512], [256,128], [64,128,256], [128,128,128], [512,256,128], [512,512,512], [1024 , 512 , 256]]\n","        },\n","        'dropout': {\n","            'values': [0 , 0.1 ,0.3 , 0.5]\n","        },\n","        'optimiser': {\n","            'values': ['adam', 'rmsprop', 'sgd']\n","        },\n","        'learning_rate': {\n","            'values': [0.0001 , 0.001, 0.002, 0.003]\n","        },\n","        'epochs': {\n","            'values': [4 , 6 , 8 , 10]\n","        },\n","        'beam_width': {\n","            'values': [1 , 2 , 3]\n","        },\n","        'batch_size':{\n","              'values': [32, 64, 128, 512]\n","        }\n","}\n","\n","sweep_config['parameters']=parameter_dict\n","import pprint\n","\n","pprint.pprint(sweep_config)\n","sweep_id = wandb.sweep(sweep_config,project=\"Assignment3\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Executing sweep\n","wandb.agent(sweep_id, wandb_sweep, count=10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def encoder_inference_model(model,latent_dims,cell_type,encoder_layers):\n","    encoder_input = encoder_layers[0]   # Taking input layer from training encoder layer\n","    encoder_embedding_layer = encoder_layers[1] # Taking embedded layer from training encoder layer\n","    encoder_embedding_output = encoder_embedding_layer(encoder_input)\n","\n","    encoder_states = []\n","    encoder_outputs = encoder_embedding_output\n","  \n","    for i in range(len(latent_dims)):\n","        index = i+2   # first two layers are input and embedded layers and remaining layers are recurrent cells\n","        encoder_recurrent_layer = encoder_layers[index]\n","        if cell_type == 'lstm':\n","            encoder_outputs, state_h, state_c = encoder_recurrent_layer(encoder_outputs)\n","            encoder_states += [state_h, state_c]  # storing states from cell to give as initial state to respective  (lstm)\n","        else:\n","            encoder_outputs, state_h = encoder_recurrent_layer(encoder_outputs)\n","            encoder_states += [state_h] # storing states from cell to give as initial state to respective  (rnn,gru)\n","\n","    encoder_inference_Model = keras.Model(inputs = encoder_input, outputs = encoder_states + [encoder_outputs]) # outputting saved encoder states and final encoder outpust\n","    \n","    return encoder_inference_Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}

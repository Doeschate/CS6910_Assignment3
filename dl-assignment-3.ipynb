{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lkYZtD_Le3Un","trusted":true},"outputs":[],"source":["!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n","!tar -xvf ./dakshina_dataset_v1.0.tar"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:10.806081Z","iopub.status.busy":"2022-05-03T05:30:10.805747Z","iopub.status.idle":"2022-05-03T05:30:17.882998Z","shell.execute_reply":"2022-05-03T05:30:17.882011Z","shell.execute_reply.started":"2022-05-03T05:30:10.806036Z"},"id":"KWqvKpoee3U4","trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Flatten,Embedding,Dense\n","from keras.utils.vis_utils import plot_model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T02:56:47.220363Z","iopub.status.busy":"2022-05-03T02:56:47.219898Z","iopub.status.idle":"2022-05-03T02:57:03.250744Z","shell.execute_reply":"2022-05-03T02:57:03.249769Z","shell.execute_reply.started":"2022-05-03T02:56:47.220317Z"},"id":"72tH1ltv_rcQ","outputId":"c602192d-d683-47ae-bbbe-b4a03f192e71","trusted":true},"outputs":[],"source":["!pip install wandb\n","# wandb login\n","import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:17.885526Z","iopub.status.busy":"2022-05-03T05:30:17.884793Z","iopub.status.idle":"2022-05-03T05:30:17.891666Z","shell.execute_reply":"2022-05-03T05:30:17.890800Z","shell.execute_reply.started":"2022-05-03T05:30:17.885472Z"},"id":"jpy9bz8we3U-","trusted":true},"outputs":[],"source":["train_path = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n","val_path =   \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n","test_path = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n","\n","def readData(path):    \n","    trainingData_df = pd.read_csv(path, sep='\\t',on_bad_lines='skip',header=None)\n","    trainingData = trainingData_df.values.tolist()\n","    return trainingData"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:18.712220Z","iopub.status.busy":"2022-05-03T05:30:18.711329Z","iopub.status.idle":"2022-05-03T05:30:18.973695Z","shell.execute_reply":"2022-05-03T05:30:18.972552Z","shell.execute_reply.started":"2022-05-03T05:30:18.712168Z"},"id":"wMozSXGHe3VE","outputId":"4467e400-d26f-424c-90d4-5a64f775643b","trusted":true},"outputs":[],"source":["# Analysing dataset\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","\n","trainingData = readData(train_path)\n","for line in trainingData:\n","    input_text, target_text = line[1],line[0]\n","    if not isinstance(input_text,str):\n","        continue\n","    target_text = \" \" + target_text + \" \"\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","input_characters.add(' ')\n","target_characters.add(' ')\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n","\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n","\n","\n","print(input_token_index)\n","print(target_token_index)\n","\n","print(\"Number of samples:\", len(input_texts))\n","num_samples = len(input_texts)\n","print(\"Number of unique input tokens:\", num_encoder_tokens)\n","print(\"Number of unique output tokens:\", num_decoder_tokens)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:20.125127Z","iopub.status.busy":"2022-05-03T05:30:20.122611Z","iopub.status.idle":"2022-05-03T05:30:20.141120Z","shell.execute_reply":"2022-05-03T05:30:20.140124Z","shell.execute_reply.started":"2022-05-03T05:30:20.125079Z"},"id":"HLGa7pmje3VH","trusted":true},"outputs":[],"source":["# Character encoding using Embedding layer....\n","\n","# Encoder inputs embedding (Latin)\n","def getData(path):\n","    print(path)\n","    input_texts = []\n","    target_texts = []\n","    data = readData(path)\n","    for line in data:\n","        input_text, target_text = line[1],line[0]\n","        if not isinstance(input_text,str):\n","            continue\n","        target_text = \" \" + target_text + \" \"\n","        input_texts.append(input_text)\n","        target_texts.append(target_text)\n","    \n","    vocab_size = num_encoder_tokens\n","    max_length = max_encoder_seq_length\n","\n","    EncoderInputEncodedWords = []\n","    for i,eachText in enumerate(input_texts):\n","        EncoderInputEncodedWords.append([])\n","        for eachChar in eachText:\n","            EncoderInputEncodedWords[i].append(input_token_index[eachChar])\n","\n","    EncoderInputEncodedWords = pad_sequences(EncoderInputEncodedWords,maxlen=max_length,padding='post',value=0.0)\n","    print('EncoderInputEncodedWords.shape',EncoderInputEncodedWords.shape)\n","    print(EncoderInputEncodedWords[:10])\n","\n","    vocab_size = num_decoder_tokens\n","    max_length = max_decoder_seq_length\n","\n","    DecoderInputEncodedWords = []\n","    for i,eachText in enumerate(target_texts):\n","        DecoderInputEncodedWords.append([])\n","        for j,eachChar in enumerate(eachText):\n","            DecoderInputEncodedWords[i].append(target_token_index[eachChar])\n","\n","    DecoderInputEncodedWords = pad_sequences(DecoderInputEncodedWords,maxlen = max_decoder_seq_length ,padding='post',value = 0.0)#max(num_decoder_tokens,num_encoder_tokens))\n","    print('DecoderInputEncodedWords.shape',DecoderInputEncodedWords.shape)\n","    print(DecoderInputEncodedWords[:10])\n","\n","    decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n","    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","        for t, char in enumerate(target_text):\n","            if t > 0:\n","                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n","\n","    with np.printoptions(threshold=np.inf):\n","      print(decoder_target_data[0])\n","    \n","    return EncoderInputEncodedWords,DecoderInputEncodedWords,decoder_target_data,input_texts,target_texts"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:26.577682Z","iopub.status.busy":"2022-05-03T05:30:26.577327Z","iopub.status.idle":"2022-05-03T05:30:28.307514Z","shell.execute_reply":"2022-05-03T05:30:28.306545Z","shell.execute_reply.started":"2022-05-03T05:30:26.577648Z"},"id":"8Ugu7H4le3VR","trusted":true},"outputs":[],"source":["encoder_input_train_data, decoder_input_train_data, decoder_target_train_data,train_eng,train_hin = getData(train_path)\n","encoder_input_val_data, decoder_input_val_data, decoder_target_val_data,val_eng,val_hin = getData(val_path)\n","encoder_input_test_data, decoder_input_test_data, decoder_target_test_data,test_eng,test_hin = getData(test_path)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}

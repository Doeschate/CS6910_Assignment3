{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lkYZtD_Le3Un","trusted":true},"outputs":[],"source":["#Downloading the dataset from the link using wget\n","!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n","!tar -xvf ./dakshina_dataset_v1.0.tar"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:10.806081Z","iopub.status.busy":"2022-05-03T05:30:10.805747Z","iopub.status.idle":"2022-05-03T05:30:17.882998Z","shell.execute_reply":"2022-05-03T05:30:17.882011Z","shell.execute_reply.started":"2022-05-03T05:30:10.806036Z"},"id":"KWqvKpoee3U4","trusted":true},"outputs":[],"source":["#importing required Libraries\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Flatten,Embedding,Dense\n","from keras.utils.vis_utils import plot_model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T02:56:47.220363Z","iopub.status.busy":"2022-05-03T02:56:47.219898Z","iopub.status.idle":"2022-05-03T02:57:03.250744Z","shell.execute_reply":"2022-05-03T02:57:03.249769Z","shell.execute_reply.started":"2022-05-03T02:56:47.220317Z"},"id":"72tH1ltv_rcQ","outputId":"c602192d-d683-47ae-bbbe-b4a03f192e71","trusted":true},"outputs":[],"source":["!pip install wandb\n","# wandb login\n","import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:17.885526Z","iopub.status.busy":"2022-05-03T05:30:17.884793Z","iopub.status.idle":"2022-05-03T05:30:17.891666Z","shell.execute_reply":"2022-05-03T05:30:17.890800Z","shell.execute_reply.started":"2022-05-03T05:30:17.885472Z"},"id":"jpy9bz8we3U-","trusted":true},"outputs":[],"source":["train_path = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n","val_path =   \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n","test_path = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n","# Parsing the training data to find number of unique characters in input language and output language( these values are useful in creating embedding layer)\n","# readData function will take path as argument and return the data present in that path as pandas DataFrame\n","def readData(path):    \n","    trainingData_df = pd.read_csv(path, sep='\\t',on_bad_lines='skip',header=None)\n","    trainingData = trainingData_df.values.tolist()\n","    return trainingData"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:18.712220Z","iopub.status.busy":"2022-05-03T05:30:18.711329Z","iopub.status.idle":"2022-05-03T05:30:18.973695Z","shell.execute_reply":"2022-05-03T05:30:18.972552Z","shell.execute_reply.started":"2022-05-03T05:30:18.712168Z"},"id":"wMozSXGHe3VE","outputId":"4467e400-d26f-424c-90d4-5a64f775643b","trusted":true},"outputs":[],"source":["# Analysing dataset\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","\n","trainingData = readData(train_path)\n","for line in trainingData:\n","    input_text, target_text = line[1],line[0]\n","    if not isinstance(input_text,str):\n","        continue\n","    target_text = \" \" + target_text + \" \"\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","input_characters.add(' ')\n","target_characters.add(' ')\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n","\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n","\n","\n","print(input_token_index)\n","print(target_token_index)\n","\n","print(\"Number of samples:\", len(input_texts))\n","num_samples = len(input_texts)\n","print(\"Number of unique input tokens:\", num_encoder_tokens)\n","print(\"Number of unique output tokens:\", num_decoder_tokens)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:20.125127Z","iopub.status.busy":"2022-05-03T05:30:20.122611Z","iopub.status.idle":"2022-05-03T05:30:20.141120Z","shell.execute_reply":"2022-05-03T05:30:20.140124Z","shell.execute_reply.started":"2022-05-03T05:30:20.125079Z"},"id":"HLGa7pmje3VH","trusted":true},"outputs":[],"source":["# Character encoding using Embedding layer....\n","\n","# Encoder inputs embedding (Latin)\n","def getData(path):\n","    print(path)\n","    input_texts = []\n","    target_texts = []\n","    data = readData(path)\n","    for line in data:\n","        input_text, target_text = line[1],line[0]\n","        if not isinstance(input_text,str):\n","            continue\n","        target_text = \" \" + target_text + \" \"\n","        input_texts.append(input_text)\n","        target_texts.append(target_text)\n","    \n","    vocab_size = num_encoder_tokens\n","    max_length = max_encoder_seq_length\n","\n","    EncoderInputEncodedWords = []\n","    for i,eachText in enumerate(input_texts):\n","        EncoderInputEncodedWords.append([])\n","        for eachChar in eachText:\n","            EncoderInputEncodedWords[i].append(input_token_index[eachChar])\n","\n","    EncoderInputEncodedWords = pad_sequences(EncoderInputEncodedWords,maxlen=max_length,padding='post',value=0.0)\n","    print('EncoderInputEncodedWords.shape',EncoderInputEncodedWords.shape)\n","    print(EncoderInputEncodedWords[:10])\n","\n","    vocab_size = num_decoder_tokens\n","    max_length = max_decoder_seq_length\n","\n","    DecoderInputEncodedWords = []\n","    for i,eachText in enumerate(target_texts):\n","        DecoderInputEncodedWords.append([])\n","        for j,eachChar in enumerate(eachText):\n","            DecoderInputEncodedWords[i].append(target_token_index[eachChar])\n","\n","    DecoderInputEncodedWords = pad_sequences(DecoderInputEncodedWords,maxlen = max_decoder_seq_length ,padding='post',value = 0.0)#max(num_decoder_tokens,num_encoder_tokens))\n","    print('DecoderInputEncodedWords.shape',DecoderInputEncodedWords.shape)\n","    print(DecoderInputEncodedWords[:10])\n","\n","    decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n","    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","        for t, char in enumerate(target_text):\n","            if t > 0:\n","                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n","\n","    with np.printoptions(threshold=np.inf):\n","      print(decoder_target_data[0])\n","    \n","    return EncoderInputEncodedWords,DecoderInputEncodedWords,decoder_target_data,input_texts,target_texts"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-03T05:30:26.577682Z","iopub.status.busy":"2022-05-03T05:30:26.577327Z","iopub.status.idle":"2022-05-03T05:30:28.307514Z","shell.execute_reply":"2022-05-03T05:30:28.306545Z","shell.execute_reply.started":"2022-05-03T05:30:26.577648Z"},"id":"8Ugu7H4le3VR","trusted":true},"outputs":[],"source":["encoder_input_train_data, decoder_input_train_data, decoder_target_train_data,train_eng,train_hin = getData(train_path)\n","encoder_input_val_data, decoder_input_val_data, decoder_target_val_data,val_eng,val_hin = getData(val_path)\n","encoder_input_test_data, decoder_input_test_data, decoder_target_test_data,test_eng,test_hin = getData(test_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define an input sequence and process it.\n","def buildModel(latent_dims,EmbeddingOutputDimensions,layer_type,dropout, lr, optimiser):\n","    encoder_inputs = keras.Input(shape=(max_encoder_seq_length,))\n","    embedding_encoder_layer = Embedding(input_dim = num_encoder_tokens, output_dim = EmbeddingOutputDimensions[0], input_length = max_encoder_seq_length,trainable=True)\n","    embedding_encoder_inuts = embedding_encoder_layer(encoder_inputs)\n","    encoder_outputs = embedding_encoder_inuts\n","    \n","    encoder_states = []\n","    encoder_layers = []\n","    encoder_layers.append(encoder_inputs)\n","    encoder_layers.append(embedding_encoder_layer)\n","    for i in range(len(latent_dims))[::-1]:\n","        if layer_type == 'lstm':\n","            encoder_layers.append(keras.layers.LSTM(latent_dims[i], return_state=True,return_sequences=True,dropout=dropout))\n","            encoder_outputs, state_h, state_c = encoder_layers[-1](encoder_outputs)\n","            encoder_states += [state_h, state_c]\n","        if layer_type == 'gru':\n","            encoder_layers.append(keras.layers.GRU(latent_dims[i], return_state=True, return_sequences=True,dropout=dropout))\n","            encoder_outputs, state_h= encoder_layers[-1](encoder_outputs)\n","            encoder_states += [state_h]\n","        if layer_type == 'rnn':\n","            encoder_layers.append(keras.layers.SimpleRNN(latent_dims[i], return_state=True, return_sequences=True,dropout=dropout))\n","            encoder_outputs, state_h = encoder_layers[-1](encoder_outputs)\n","            encoder_states += [state_h]\n","\n","    decoder_inputs = keras.Input(shape=(max_decoder_seq_length,))\n","    embedding_decoder_layer = Embedding(input_dim = num_decoder_tokens, output_dim = EmbeddingOutputDimensions[1], input_length = max_decoder_seq_length,trainable=True)\n","    embedding_decoder_inputs = embedding_decoder_layer(decoder_inputs)\n","    decoder_outputs_temp = embedding_decoder_inputs\n","    decoder_layers = []\n","    decoder_layers.append(decoder_inputs)\n","    decoder_layers.append(embedding_decoder_layer)\n","\n","    for i in range(len(latent_dims)):\n","        if layer_type == 'lstm':\n","            layer = keras.layers.LSTM(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True,dropout=dropout)\n","            decoder_outputs_temp, dh, dc = layer(decoder_outputs_temp, initial_state=encoder_states[2*i:2*(i+1)])\n","            decoder_layers.append(layer)\n","        if layer_type == 'gru':\n","            layer = keras.layers.GRU(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True,dropout=dropout)\n","            decoder_outputs_temp,dh = layer(decoder_outputs_temp, initial_state=encoder_states[i])\n","            decoder_layers.append(layer)\n","        if layer_type == 'rnn':\n","            layer = keras.layers.SimpleRNN(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True,dropout=dropout)\n","            decoder_outputs_temp, dh = layer(decoder_outputs_temp, initial_state=encoder_states[i])\n","            decoder_layers.append(layer)\n","            \n","    if attention == True:\n","        attention_layer = keras.layers.AdditiveAttention() \n","        decoder_layers.append(attention_layer)\n","        attn_out, attn_states = attention_layer([ decoder_outputs_temp , encoder_outputs] , return_attention_scores = True) \n","\n","        concatenate_layer = Concatenate(axis=-1, name='concat_layer')\n","        decoder_layers.append(concatenate_layer)\n","        decoder_outputs_temp = concatenate_layer([decoder_outputs_temp, attn_out])\n","\n","\n","    dense_layer = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n","    decoder_outputs = dense_layer(decoder_outputs_temp) \n","    decoder_layers.append(dense_layer)\n","\n","    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","    #model.summary()\n","    plot_model(model, to_file='model12.png', show_shapes=True)\n","    \n","    lr = tf.keras.experimental.CosineDecayRestarts(lr , 1000)\n","    if optimiser == 'adam':\n","        optim = keras.optimizers.Adam(learning_rate=lr)\n","    elif optimiser == 'rmsprop':\n","        optim = keras.optimizers.RMSprop(learning_rate=lr)\n","    elif optimiser == 'sgd':\n","        optim = keras.optimizers.SGD(learning_rate=lr)\n","    model.compile(optimizer =optim, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","    \n","    return model,encoder_layers,decoder_layers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def beam_selector(logit , beam_width , rows ):\n","    holder = list()\n","    for i in range(len(rows)):\n","        cur_sequence, cur_score = rows[i]\n","        desired_top = np.argsort(logit)[-beam_width:]\n","    for desired in desired_top:\n","        current_holder = [cur_sequence + [desired], cur_score + tf.math.log(logit[desired])]\n","        holder.append(current_holder)\n","    return holder\n","\n","def BeamDecoder(logits, beam_width):\n","    rows = [[list(), 0.0]]\n","    logits  = tf.nn.softmax(logits)\n","    for logit in logits:\n","        holder = beam_selector(logit , beam_width , rows )\n","        rev_sorted = sorted(holder, key=lambda start_tuple:start_tuple[1], reverse=True)\n","        rows = rev_sorted[:beam_width]\n","    return np.array(rows)[:,0:1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AccuracyCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, trainingData, validationData, beam_width):\n","        self.trainingData = trainingData\n","        self.validationData = validationData   \n","        self.beamWidth = beam_width\n","\n","    def on_epoch_end(self, epoch , logs):\n","        train_preds = self.model.predict(self.trainingData[0])\n","        train_truth = self.trainingData[1]\n","        train_pred_one_hot = tf.one_hot(tf.argmax(train_preds, axis=2), train_preds.shape[2]).numpy()\n","        correct_word_count = 0\n","        correct_character_count = 0\n","\n","        for i in range(len(train_preds)):\n","            if np.array_equal(train_truth[i], train_pred_one_hot[i]):\n","                correct_word_count += 1\n","            for j in range(0,len(train_truth[i])):\n","                if np.array_equal(train_truth[i][j], train_pred_one_hot[i][j]):\n","                    correct_character_count += 1\n","\n","        trainingAccuracy_word = correct_word_count/len(train_preds)\n","        trainingAccuracy_character = correct_character_count/(len(train_preds)*len(train_preds[0]))\n","        print(\"\")\n","        print(correct_character_count,len(train_preds),len(train_preds[0]))\n","        print(str(correct_word_count),\" words correctly predicted among \",len(train_preds),\" words\")\n","        print(\"Training Accuracy (word): \"+ str(trainingAccuracy_word))\n","        print(\"Training Accuracy (character): \"+ str(trainingAccuracy_character))\n","\n","        #wandb.log({\"accuracy\": (correct_count/len(train_preds))})\n","\n","\n","        val_preds = self.model.predict(self.validationData[0])\n","        val_truth = self.validationData[1]\n","\n","        val_pred_one_hot = tf.one_hot(tf.argmax(val_preds, axis=2), val_preds.shape[2]).numpy()\n","        correct_word_count = 0\n","        correct_character_count = 0\n","        for i in range(len(val_preds)):\n","            req_length = len(val_hin[i].strip()) \n","            beam_result = BeamDecoder(np.array(val_preds[i][:req_length+1]) , self.beamWidth)\n","\n","            ground_label = decoder_input_val_data[i ,1:req_length+2]\n","\n","            maxCharMatch = 0\n","            for beam in beam_result:\n","                charMatch = 0  \n","                for j in range(0,len(ground_label)):\n","                    if beam[0][j] == ground_label[j]:\n","                        charMatch += 1\n","                    if charMatch > maxCharMatch:\n","                        maxCharMatch = charMatch\n","\n","                if(np.array_equal(beam[0] , ground_label)):\n","                    correct_word_count += 1\n","                    break\n","            correct_character_count +=  maxCharMatch\n","        validationAccuracy_word = (correct_word_count/len(val_preds))\n","        validationAccuracy_char = (correct_character_count/(len(val_preds)*len(val_preds[0])))\n","        print(correct_character_count,len(val_preds),len(val_preds[0]))\n","        print(str(correct_word_count),\" words correctly predicted among \",len(val_preds),\" words\")\n","        print(\"Validation Accuracy (word) : \"+ str(validationAccuracy_word))\n","        print(\"Validation Accuracy (character): \"+ str(validationAccuracy_char))\n","\n","        wandb.log({ \"epoch\": epoch,\"accuracy\": trainingAccuracy_word,\"val_accuracy\": validationAccuracy_word,\"accuracy (character)\": trainingAccuracy_character, \"val_accuracy (character)\": validationAccuracy_char})\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["latent_dims = [128,128,128]\n","embed_dims = [128,256]\n","attention = False\n","cell_type = 'gru'\n","dropout = 0.1\n","batch_size = 64  # Batch size for training.\n","epochs = 20  # Number of epochs to train for.\n","learning_rate = 0.002\n","optimiser = 'rmsprop'\n","beam_width = 3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model, encoder_layers, decoder_layers = buildModel(latent_dims=latent_dims,EmbeddingOutputDimensions=embed_dims,layer_type=cell_type,dropout=dropout, lr=learning_rate, optimiser=optimiser)\n","\n","acc_callback = AccuracyCallback(trainingData = ([encoder_input_train_data, decoder_input_train_data], decoder_target_train_data),validationData = ([encoder_input_val_data, decoder_input_val_data], decoder_target_val_data), beam_width=beam_width)\n","\n","model.fit([encoder_input_train_data, decoder_input_train_data],decoder_target_train_data,batch_size=batch_size,\n","          epochs=epochs,shuffle=True,callbacks=[acc_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def encoder_inference_model(model,latent_dims,cell_type,encoder_layers):\n","    encoder_input = encoder_layers[0]   # Taking input layer from training encoder layer\n","    encoder_embedding_layer = encoder_layers[1] # Taking embedded layer from training encoder layer\n","    encoder_embedding_output = encoder_embedding_layer(encoder_input)\n","\n","    encoder_states = []\n","    encoder_outputs = encoder_embedding_output\n","  \n","    for i in range(len(latent_dims)):\n","        index = i+2   # first two layers are input and embedded layers and remaining layers are recurrent cells\n","        encoder_recurrent_layer = encoder_layers[index]\n","        if cell_type == 'lstm':\n","            encoder_outputs, state_h, state_c = encoder_recurrent_layer(encoder_outputs)\n","            encoder_states += [state_h, state_c]  # storing states from cell to give as initial state to respective  (lstm)\n","        else:\n","            encoder_outputs, state_h = encoder_recurrent_layer(encoder_outputs)\n","            encoder_states += [state_h] # storing states from cell to give as initial state to respective  (rnn,gru)\n","\n","    encoder_inference_Model = keras.Model(inputs = encoder_input, outputs = encoder_states + [encoder_outputs]) # outputting saved encoder states and final encoder outpust\n","    \n","    return encoder_inference_Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def decoder_inference_model(model, latent_dims , embed_dims, attention ,cell_type, decoder_layers):\n","    decoder_inference_input = decoder_layers[0]  # training decoder input layer\n","    decoder_outputs = decoder_layers[1](decoder_inference_input) # training decoder embedded layer\n","    \n","    \n","    decoder_states_inputs = []\n","    decoder_states_outputs = []\n","    \n","    for i in range(len(latent_dims))[::-1]:\n","        index = len(latent_dims)-i-1+2\n","        if cell_type == 'lstm':\n","            starting_state_inputs = [keras.Input(shape=(latent_dims[i],)) for num_hidden_states in range(2)] # Encoder states as starting states\n","            decoder_outputs, state_h, state_c = decoder_layers[index](decoder_outputs, initial_state=starting_state_inputs)\n","            decoder_states_outputs += [state_h , state_c] # These states are useful generating translation of word.(as we are generating characterby character)\n","            decoder_states_inputs += starting_state_inputs\n","        else:\n","            starting_state_inputs = [keras.Input(shape=(latent_dims[i],)) for num_hidden_states in range(1)]\n","            decoder_outputs, state_h = decoder_layers[index](decoder_outputs, initial_state= starting_state_inputs)\n","            decoder_states_outputs += [state_h ]\n","            decoder_states_inputs += starting_state_inputs\n","\n","    decoder_hidden_input = keras.Input(shape=(max_encoder_seq_length , latent_dims[0]))\n","    \n","    if attention == True:\n","        attention_layer = decoder_layers[-3] \n","    \n","        attention_output, attention_states = attention_layer([ decoder_outputs , decoder_hidden_input], return_attention_scores = True) \n","        concatenation_layer = decoder_layers[-2]\n","        decoder_outputs = concatenation_layer([decoder_outputs, attention_output])\n","\n","        attention_model = keras.Model(\n","          [decoder_inference_input] + [decoder_hidden_input] + decoder_states_inputs,\n","          [attention_states])\n","\n","        decoder_dense = decoder_layers[-1]\n","        decoder_outputs = decoder_dense(decoder_outputs) \n","\n","        decoder_model = keras.Model(\n","                [decoder_inference_input]  + decoder_states_inputs + [decoder_hidden_input],\n","                [decoder_outputs] + decoder_states_outputs) \n","\n","        return  decoder_model, attention_model\n","\n","    decoder_dense = decoder_layers[-1]\n","    decoder_outputs = decoder_dense(decoder_outputs) \n","\n","    decoder_model = keras.Model(\n","          [decoder_inference_input]  + decoder_states_inputs + [decoder_hidden_input],\n","          [decoder_outputs] + decoder_states_outputs) \n","\n","    return  decoder_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def Finding_test_accuracy():\n","    correct_count = 0\n","    total_words = len(test_eng)\n","    for i in range(0,total_words):\n","        input = encoder_input_test_data[i]\n","        real_output = decoder_input_test_data[i]\n","        if(attention):\n","            prediction,attention_out = translate_word(input.reshape(1,max_encoder_seq_length))\n","        else:\n","            prediction = translate_word(input.reshape(1,max_encoder_seq_length))\n","        correct_word_length = len(test_hin[i].strip())\n","        beam_result = BeamDecoder(np.array(beamlogits) , beam_width)\n","        real_output = real_output[1:correct_word_length+2]\n","        #print(\"--------------\")\n","        #print('g',real_output)\n","        for pred in beam_result:\n","            #print('p',pred[0])\n","            if(np.array_equal(pred[0],real_output)):\n","                correct_count += 1\n","                break\n","        if i%200==0:\n","            print(\"Test accuracy without Attention: {0}, Completion = {1}\".format(correct_count*100/(i+1), i/total_words))\n","    print(\"Test accuracy : \", correct_count*100/total_words)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def translate_word(input_word):\n","    att = [] \n","    full_encoder_inference_output = encoder_model.predict(input_word)\n","    once_encoder_output = full_encoder_inference_output[-1]  # Final cell outputs\n","    recurrent_states = full_encoder_inference_output[0:-1]   # All cells hidden states \n","    global beamlogits \n","    beamlogits = []\n","    rec_target_char = np.zeros((1,1))    \n","    rec_target_char[0, 0] = target_token_index[' ']   # initial input to decoder is space character\n","\n","    decoding_stop = False\n","    decoded_word = ''\n","    while(decoding_stop == False):\n","        full_decoder_output = decoder_model.predict([rec_target_char] + recurrent_states + [once_encoder_output])\n","        beamlogits.append(full_decoder_output[0][0][0])\n","        if(attention):\n","            attention_array = attention_model([rec_target_char] + [once_encoder_output]+recurrent_states)\n","            att.append(attention_array.numpy().flatten())\n","        current_token_ind = np.argmax(full_decoder_output[0][0, -1, :])  # finding maximum probability character\n","        current_char = reverse_target_char_index[current_token_ind]\n","        if (current_char == ' ' or len(decoded_word) >  max_decoder_seq_length):  # if decoded character is ' ' or decoded sequence is longer than required then terminate\n","            decoding_stop = True\n","        else:    \n","            decoded_word += ''+ current_char\n","\n","        rec_target_char = np.zeros((1,1))\n","        rec_target_char[0, 0] = current_token_ind # making current predicted character as input to decoder in next iteration\n","\n","        recurrent_states = full_decoder_output[1:] # states preparation\n","    if (attention):\n","        return decoded_word , np.asarray(att)\n","    else:\n","        return decoded_word"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoder_model = encoder_inference_model(model,latent_dims,cell_type,encoder_layers)\n","if attention == True:\n","    decoder_model, attention_model = decoder_inference_model(model, latent_dims , embed_dims, attention ,cell_type, decoder_layers)\n","else:\n","    decoder_model = decoder_inference_model(model, latent_dims , embed_dims, attention ,cell_type, decoder_layers)\n","plot_model(encoder_model, to_file='emodel.png', show_shapes=True)\n","plot_model(decoder_model, to_file='dmodel.png', show_shapes=True)\n","\n","Finding_test_accuracy()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}

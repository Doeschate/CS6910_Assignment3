{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-05T13:28:44.819248Z","iopub.status.busy":"2022-05-05T13:28:44.818927Z","iopub.status.idle":"2022-05-05T13:28:49.804623Z","shell.execute_reply":"2022-05-05T13:28:49.803908Z","shell.execute_reply.started":"2022-05-05T13:28:44.819159Z"},"trusted":true},"outputs":[],"source":["#importing required Libraries\n","import os, shutil\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Flatten,Embedding,Dense,Concatenate\n","from keras.utils.vis_utils import plot_model\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-05T13:28:53.754009Z","iopub.status.busy":"2022-05-05T13:28:53.753363Z","iopub.status.idle":"2022-05-05T13:29:22.080374Z","shell.execute_reply":"2022-05-05T13:29:22.079758Z","shell.execute_reply.started":"2022-05-05T13:28:53.753967Z"},"trusted":true},"outputs":[],"source":["#Downloading the dataset from the link using wget\n","!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-05T13:29:25.356269Z","iopub.status.busy":"2022-05-05T13:29:25.355515Z","iopub.status.idle":"2022-05-05T13:29:29.686233Z","shell.execute_reply":"2022-05-05T13:29:29.685029Z","shell.execute_reply.started":"2022-05-05T13:29:25.356219Z"},"trusted":true},"outputs":[],"source":["#Extracting and removing the .tar file\n","!tar -xf dakshina_dataset_v1.0.tar\n","!rm -r dakshina_dataset_v1.0.tar"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-05T13:29:34.447632Z","iopub.status.busy":"2022-05-05T13:29:34.447135Z","iopub.status.idle":"2022-05-05T13:29:34.452533Z","shell.execute_reply":"2022-05-05T13:29:34.451724Z","shell.execute_reply.started":"2022-05-05T13:29:34.447601Z"},"trusted":true},"outputs":[],"source":["#Choose the language\n","# Colab\n","# dir = '/content/dakshina_dataset_v1.0'\n","# Kaggle\n","dir = './dakshina_dataset_v1.0'\n","lang = \"hi\""]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-05T13:29:43.359972Z","iopub.status.busy":"2022-05-05T13:29:43.359495Z","iopub.status.idle":"2022-05-05T13:29:43.656898Z","shell.execute_reply":"2022-05-05T13:29:43.656250Z","shell.execute_reply.started":"2022-05-05T13:29:43.359930Z"},"trusted":true},"outputs":[],"source":["#Removing the unnecessary folders and keeping only the lang choosen lexicons folder\n","for files in os.listdir(dir):\n","    if (files==lang):    \n","        path = os.path.join(dir, files)\n","        for files_lang in os.listdir(path):\n","            if(files_lang!=\"lexicons\"):\n","                path_lang = os.path.join(path, files_lang)\n","                try:\n","                    shutil.rmtree(path_lang)\n","                except OSError:\n","                    os.remove(path_lang)\n","    else:    \n","        path = os.path.join(dir, files)\n","        try:\n","            shutil.rmtree(path)\n","        except OSError:\n","            os.remove(path)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-05T13:29:46.757878Z","iopub.status.busy":"2022-05-05T13:29:46.757524Z","iopub.status.idle":"2022-05-05T13:29:46.761972Z","shell.execute_reply":"2022-05-05T13:29:46.761073Z","shell.execute_reply.started":"2022-05-05T13:29:46.757833Z"},"trusted":true},"outputs":[],"source":["#Train, validation and test dataset path\n","train_folder = \".translit.sampled.train.tsv\"\n","train_path = os.path.join(dir, lang,\"lexicons\",lang+train_folder)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-05T13:29:49.559724Z","iopub.status.busy":"2022-05-05T13:29:49.559474Z","iopub.status.idle":"2022-05-05T13:29:49.766866Z","shell.execute_reply":"2022-05-05T13:29:49.766033Z","shell.execute_reply.started":"2022-05-05T13:29:49.559696Z"},"trusted":true},"outputs":[],"source":["# Parsing the training data to find number of unique characters in input language and output language( these values are useful in creating embedding layer)\n","# readData function will take path as argument and return the data present in that path as pandas DataFrame\n","def readData(path):    \n","    trainingData_df = pd.read_csv(path, sep='\\t',on_bad_lines='skip',header=None)\n","    trainingData = trainingData_df.values.tolist()\n","    return trainingData\n","\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","\n","# Reading the training Data as data frame\n","trainingData = readData(train_path)\n","\n","# Iterating through each training data item\n","for line in trainingData:\n","    input_text, target_text = line[1],line[0]\n","    if not isinstance(input_text,str):\n","        continue\n","    target_text = \" \" + target_text + \" \"\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","# input_characters list contain unique characters in input language \n","input_characters.add(' ')\n","# target_characters list contain unique characters in target language \n","target_characters.add(' ')\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","\n","\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","# max_encoder_seq_length is maximum length word in input language\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","# max_decoder_seq_length is maximum length word in target language\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print(\"Number of unique input tokens:\", num_encoder_tokens)\n","print(\"Number of unique output tokens:\", num_decoder_tokens)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-05T13:29:52.697971Z","iopub.status.busy":"2022-05-05T13:29:52.697258Z","iopub.status.idle":"2022-05-05T13:29:52.713236Z","shell.execute_reply":"2022-05-05T13:29:52.712425Z","shell.execute_reply.started":"2022-05-05T13:29:52.697924Z"},"trusted":true},"outputs":[],"source":["'''\n","builModel will take \n","    \n","    latent_dims - list of hidden states sizes, list length gives number of encoders and decoder present in the model\n","    EmbeddingOutputDimensions - [encoder embedding output size, decoder embedding output size]\n","    layer_type - type of cell used in encoder and decoder (rnn or lstm or gru)\n","    \n","    as arguments and return prepared model\n","'''\n","\n","def buildModel(latent_dims,EmbeddingOutputDimensions,layer_type):\n","    encoder_inputs = keras.Input(shape=(None,)) # input layer\n","    embedding_encoder_layer = Embedding(input_dim = num_encoder_tokens, output_dim = EmbeddingOutputDimensions[0], trainable=True) # embedding layer\n","    embedding_encoder_inuts = embedding_encoder_layer(encoder_inputs)\n","    encoder_outputs = embedding_encoder_inuts\n","    \n","    encoder_states = [] # encoder_states stores last hidden states return by each cell in encoder to provide as input to respective decoder cells \n","    \n","    # this loop creats layer_type cells with configuration present in latent_dims list\n","    for i in range(len(latent_dims))[::-1]:\n","        if layer_type == 'lstm':\n","            encoder_outputs, state_h, state_c = keras.layers.LSTM(latent_dims[i], return_state=True,return_sequences=True)(encoder_outputs)\n","            encoder_states += [state_h, state_c]\n","        if layer_type == 'gru':\n","            encoder_outputs, state_h= keras.layers.GRU(latent_dims[i], return_state=True, return_sequences=True)(encoder_outputs)\n","            encoder_states += [state_h]\n","        if layer_type == 'rnn':\n","            encoder_outputs, state_h = keras.layers.SimpleRNN(latent_dims[i], return_state=True, return_sequences=True)(encoder_outputs)\n","            encoder_states += [state_h]\n","\n","    decoder_inputs = keras.Input(shape=(None,)) # input layer\n","    embedding_decoder_layer = Embedding(input_dim = num_decoder_tokens, output_dim = EmbeddingOutputDimensions[1],trainable=True) # embedding layer\n","    embedding_decoder_inputs = embedding_decoder_layer(decoder_inputs)\n","    decoder_outputs_temp = embedding_decoder_inputs\n","    \n","    # this loop creats layer_type cells with configuration present in latent_dims list\n","    for i in range(len(latent_dims)):\n","        if layer_type == 'lstm':\n","            decoder_outputs_temp, dh, dc = keras.layers.LSTM(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True)(decoder_outputs_temp, initial_state=encoder_states[2*i:2*(i+1)])\n","        if layer_type == 'gru':\n","            decoder_outputs_temp,dh = keras.layers.GRU(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True)(decoder_outputs_temp, initial_state=encoder_states[i])\n","        if layer_type == 'rnn':\n","            decoder_outputs_temp, dh = keras.layers.SimpleRNN(latent_dims[len(latent_dims) - i - 1], return_sequences=True, return_state=True)(decoder_outputs_temp, initial_state=encoder_states[i])\n","     \n","    dense_layer = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\") #dense layer\n","    decoder_outputs = dense_layer(decoder_outputs_temp) \n","    \n","    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs) # creating model\n","    \n","    return model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-05T13:29:55.636205Z","iopub.status.busy":"2022-05-05T13:29:55.635419Z","iopub.status.idle":"2022-05-05T13:29:57.625436Z","shell.execute_reply":"2022-05-05T13:29:57.624760Z","shell.execute_reply.started":"2022-05-05T13:29:55.636164Z"},"trusted":true},"outputs":[],"source":["'''\n","Format of command line arguments\n","1) number of cells in encoder or decoder (let n)\n","2 to n) hidden states sizes\n","n+1) encoder embedding output size\n","n+2) decoder embedding output size\n","n+3) cell type (rnn or lstm or gru)\n","'''\n","# number_of_en_de = int(sys.argv[1])\n","# latent_dims = []\n","# for i in range(0,number_of_en_de):\n","#     latent_dims.append(int(sys.argv[i+2]))\n","# embed_dims = [int(sys.argv[number_of_en_de+2]),int(sys.argv[number_of_en_de+3])]\n","# cell_type = sys.argv(number_of_en_de+4)\n","\n","\n","number_of_en_de = 3\n","latent_dims = [256,512,256]\n","embed_dims = [256,256]\n","cell_type = 'lstm'\n","\n","\n","# build model\n","model = buildModel(latent_dims=latent_dims,EmbeddingOutputDimensions=embed_dims,layer_type=cell_type)\n","model.summary()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-05T13:30:04.237537Z","iopub.status.busy":"2022-05-05T13:30:04.237302Z","iopub.status.idle":"2022-05-05T13:30:05.193739Z","shell.execute_reply":"2022-05-05T13:30:05.192934Z","shell.execute_reply.started":"2022-05-05T13:30:04.237512Z"},"trusted":true},"outputs":[],"source":["plot_model(model)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
